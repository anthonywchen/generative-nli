{
    "anli": {
        "accuracy": "26.9 +- 0.3",
        "accuracy_r1_test": "23.1 +- 0.9",
        "accuracy_r2_test": "27.4 +- 0.8",
        "accuracy_r3_test": "29.6 +- 0.6"
    },
    "bizarro": {
        "accuracy": "68.6 +- 1.0",
        "accuracy_raw_data/bizarro/revised_hypothesis": "76.1 +- 0.9",
        "accuracy_raw_data/bizarro/revised_premise": "61.0 +- 1.2"
    },
    "hans": {
        "accuracy": "56.6 +- 3.7",
        "accuracy_constituent_entailment": "99.0 +- 0.7",
        "accuracy_constituent_not_entailment": "15.5 +- 7.8",
        "accuracy_lexical_overlap_entailment": "96.4 +- 1.8",
        "accuracy_lexical_overlap_not_entailment": "24.6 +- 15.1",
        "accuracy_subsequence_entailment": "99.5 +- 0.3",
        "accuracy_subsequence_not_entailment": "4.4 +- 2.8"
    },
    "mnli_dev": {
        "accuracy": "84.6 +- 0.2"
    },
    "rte": {
        "accuracy": "75.2 +- 0.5",
        "accuracy_dev": "74.5 +- 1.5",
        "accuracy_train": "75.2 +- 0.5"
    },
    "scitail": {
        "accuracy": "79.0 +- 0.2"
    }
}